# 산업안전 CCTV 캡스톤 설계 정리

## 1. 목표 기능 목록
1) 사람 인식
- 영상에서 사람을 실시간 검출
- 같은 사람을 프레임 간 추적해서 이벤트 중복을 줄임

2) 안전 장구류 착용 판별
- 예: 안전모, 안전조끼, 안전화, 보안경, 장갑 등
- 카메라 각도와 해상도에 따라 현실적으로 가능한 항목부터 선택

3) 자세 및 행동 기반 위험 감지
- 예: 넘어짐, 쓰러짐, 장시간 누움, 위험한 자세(허리 과굴곡 등), 기계 주변 위험 행동
- 자세는 포즈 추정 후 룰 기반으로 1차 구현이 빠름

4) 위험 구역 설정 및 진입 감지
- 카메라별로 다각형 폴리곤으로 위험 구역을 설정
- 사람 바운딩 박스의 바닥점(예: bbox 하단 중앙)이 폴리곤 내부로 들어오면 침입으로 판단

5) 경고 이벤트 생성 및 전달
- AI가 이벤트를 만들고 백엔드로 전달
- 프론트는 실시간 알림과 이력 조회
- IoT는 사이렌, 경광등, 방송 등 트리거

---

## 2. 팀 구성 기준 역할 분담
AI(너)
- RTSP 프레임 수신 후 추론
- 사람 검출, 장구류 판별, 포즈 추정, 위험구역 침입 판단
- 이벤트 생성 규칙, 중복 억제, 스냅샷 저장
- 백엔드로 이벤트 전송 API 클라이언트

스트리밍 담당
- RTSP 송출 환경 구성(카메라 또는 NVR)
- 코덱, 해상도, 프레임레이트, 지연 튜닝
- 필요 시 RTSP 게이트웨이(예: ffmpeg, gstreamer) 구성

IoT 담당
- 경광등, 사이렌, 릴레이, 스피커 등 출력 장치 제어
- MQTT 등으로 백엔드 명령을 받아 동작
- 현장 장치 상태(온라인, 오류, 수동모드) 보고

프론트 담당
- 카메라 목록, 라이브 화면 보기
- 위험구역 폴리곤 편집 UI
- 이벤트 알림 화면, 이력 조회, 필터링

백엔드 담당
- 사용자, 카메라, 위험구역, 이벤트 저장
- 실시간 알림(웹소켓)
- IoT 제어 명령 라우팅
- 파일 저장소 연동(스냅샷, 짧은 클립)

---

## 3. 전체 아키텍처 추천(캡스톤용 현실 버전)

### 구성 요소
1) 카메라 또는 NVR
- RTSP로 영상 제공

2) AI Inference 서비스(파이썬)
- RTSP 수신
- 추론 및 이벤트 판단
- 이벤트를 백엔드로 전송
- 스냅샷 저장

3) 백엔드 API
- 이벤트 수신, 저장
- 프론트에 실시간 푸시
- IoT에 경고 명령 전송

4) 프론트 웹
- 실시간 이벤트 표시
- 위험구역 설정, 카메라 관리, 이력 조회

5) IoT 디바이스
- 백엔드 명령 수신 후 경고 장치 구동

### 데이터 흐름(텍스트 시퀀스)
- 카메라 -> RTSP -> AI
- AI -> 이벤트 JSON -> 백엔드
- 백엔드 -> 웹소켓 -> 프론트 알림
- 백엔드 -> MQTT 또는 HTTP -> IoT 경고

---

## 4. 프로토콜 정하기(팀 간 합의 포인트)

### 4.1 카메라 영상
- RTSP를 기본으로 추천
- AI에서 OpenCV VideoCapture 또는 gstreamer 파이프라인 사용
- 합의해야 할 것
  - 해상도: 720p 또는 1080p
  - FPS: 10~15로 제한하면 추론 안정적
  - 코덱: H.264 권장
  - 인증: rtsp://user:pass@ip:port/stream 형태 여부

### 4.2 AI -> 백엔드 이벤트 전송
캡스톤에서 제일 쉬운 선택
- HTTP REST: /api/events 로 POST
- 백엔드에서 받자마자 DB 저장, 웹소켓 푸시

실시간성이 더 필요하면
- AI -> 백엔드: gRPC 또는 메시지큐(Kafka, RabbitMQ)
- 캡스톤이면 REST로도 충분

### 4.3 백엔드 -> 프론트 실시간 알림
- WebSocket 또는 SSE(Server-Sent Events)
- 이벤트 발생 즉시 화면에 팝업, 사이드바 누적

### 4.4 백엔드 -> IoT 제어
추천 1순위
- MQTT
- topic 예시
  - site/{site_id}/device/{device_id}/cmd
  - site/{site_id}/device/{device_id}/status

간단 버전
- IoT가 HTTP로 폴링하거나, 백엔드가 HTTP로 호출

---

## 5. 이벤트 메시지 스펙(팀 전체가 공유해야 함)

### 5.1 공통 이벤트 JSON 예시
```json
{
  "event_id": "uuid",
  "ts": "2026-01-28T14:03:21+09:00",
  "site_id": "S001",
  "camera_id": "C012",
  "event_type": "ZONE_INTRUSION",
  "severity": "HIGH",
  "track_id": 37,
  "bbox": { "x1": 120, "y1": 80, "x2": 260, "y2": 410 },
  "confidence": 0.91,
  "ppe": { "helmet": false, "vest": true, "gloves": "unknown" },
  "pose": { "fall": false, "lying": false, "risk_score": 0.2 },
  "zone": { "zone_id": "Z3", "inside": true },
  "snapshot_url": "/files/snapshots/S001/C012/20260128_140321_37.jpg",
  "note": "entered restricted area"
}
````

### 5.2 이벤트 타입 예시 목록

* PPE_MISSING
* ZONE_INTRUSION
* FALL_DETECTED
* LYING_LONG
* UNSAFE_POSTURE
* CROWDING(선택)

### 5.3 중복 억제 규칙(중요)

* 같은 camera_id, track_id, event_type 조합은 쿨다운 시간을 둠
* 예: 10초 동안 같은 타입 이벤트는 1번만 생성
* severity가 바뀌면 즉시 재발행 가능

---

## 6. DB에 무엇을 저장할지(백엔드와 합의)

최소 스키마(핵심만)

* sites
* cameras
* zones
* events
* devices(iot)
* users(선택)

events에 저장할 것

* ts, site_id, camera_id
* event_type, severity
* track_id
* bbox, confidence
* ppe 상태, zone_id, snapshot_path
* raw_payload(JSON)도 같이 저장하면 나중에 확장 쉬움

스냅샷 저장 방식

* 파일 시스템 또는 오브젝트 스토리지(MinIO, S3)
* DB에는 경로만 저장

---

## 7. AI 모듈 설계(너의 범위)

### 7.1 파이프라인 구성

1. 프레임 수신

* RTSP 입력
* 프레임 드랍 정책(예: 1초에 10프레임만 처리)

2. 사람 검출

* 모델: 객체 검출기
* 출력: 사람 bbox 리스트

3. 추적

* ByteTrack 또는 DeepSORT
* 출력: track_id가 붙은 사람 bbox

4. 장구류 판별
   선택지 A(단순)

* PPE를 객체 검출로 같이 학습(사람, 헬멧, 조끼 등)
* 사람 bbox 내부에 헬멧 bbox가 있으면 착용으로 판정

선택지 B(정확도 우선)

* 사람 bbox crop 후 PPE 분류 모델을 따로 돌림
* 장구류 종류별로 멀티라벨 분류

5. 자세 추정 및 룰 기반 판단

* 포즈 추정 모델로 관절 좌표 추출
* 룰로 fall, lying, unsafe posture 판단

  * 예: 몸통 기울기, 머리-엉덩이-발 위치 관계, 관절 각도 임계값

6. 위험구역 침입 판단

* zone 폴리곤(픽셀 좌표) 로딩
* 사람의 바닥점이 폴리곤 내부인지 체크

7. 이벤트 생성 및 전송

* 중복 억제 적용
* 스냅샷 저장
* REST로 백엔드 전송

---

## 8. 모델 선택 가이드(캡스톤 현실 기준)

### 8.1 사람 검출

* 경량 모델 추천(실시간 중요)
* 추적을 붙이면 이벤트 품질이 좋아짐

### 8.2 PPE 검출

* 공개 데이터셋 + 추가 수집이 현실적
* 현장 카메라 각도, 조도, 거리에서 성능이 확 떨어질 수 있음
* 우선 안전모, 조끼 2종부터 시작 추천

### 8.3 포즈 추정

* 포즈는 룰 기반 구현이 빠르고 데모하기 좋음
* 완전 딥러닝 기반 fall detection도 가능하지만 데이터 수집이 어려움

---

## 9. 데이터셋과 라이선스(상용 사용 여부 핵심 정리)

### 9.1 결론부터

* 상용 사용 가능 여부는 2가지를 모두 확인해야 함

  1. 데이터셋 라이선스(학습에 쓴 데이터)
  2. 모델 코드와 가중치 라이선스(프레임워크, 레포, 사전학습 가중치)

캡스톤(학교 과제)에서는 보통 연구, 교육 목적 사용이 가능하지만,
졸업 후 외부에 서비스로 배포하거나 기업 PoC로 넘길 생각이면 라이선스가 중요해짐.

### 9.2 안전하게 가는 방법

* 직접 촬영한 데이터 + 직접 라벨링 데이터로 학습
* 모델은 상용 친화 라이선스인 것 선택
* 사용한 모든 오픈소스, 데이터셋을 문서로 남김(OSS notice)

### 9.3 공개 데이터셋을 쓸 때 체크리스트

* 데이터셋 페이지에서 license 항목 확인
* 상업적 사용 가능(commercial use) 표기 확인
* 2차 배포 가능 여부(redistribution) 확인
* 이미지에 사람이 나오면 초상권, 개인정보 이슈를 피하기 위해

  * 원본 공개를 하지 않고
  * 내부 학습용으로만 사용하고
  * 결과물 데모에서는 얼굴 모자이크 옵션 제공 권장

### 9.4 이미 나와있는 모델을 써도 되나

* 가능하지만 라이선스가 제약될 수 있음
* 특히 코드가 특정 강한 카피레프트 라이선스이면

  * 네가 만든 시스템 전체 공개 의무가 생길 수 있음(상황에 따라)
* 그래서 캡스톤은 일단 오픈소스로 구성하고,
  상용화 가능성까지 고려하면 라이선스가 느슨한 구성으로 재정리하는 전략이 좋음

---

## 10. 위험구역 설정 UI와 좌표 합의(프론트, 백엔드, AI가 맞춰야 함)

필수 합의

* zone 좌표는 어떤 기준인가

  * 카메라 프레임 픽셀 좌표 기준(가장 단순)
  * 예: (x, y) 는 영상 좌상단을 (0,0) 으로 하는 픽셀

추천 방식

* 프론트에서 영상 스냅샷 위에 폴리곤을 찍고
* 백엔드에 zone을 저장
* AI는 camera_id로 zones를 불러와서 사용

zone 데이터 예시

```json
{
  "zone_id": "Z3",
  "camera_id": "C012",
  "name": "forklift_lane",
  "polygon": [[120,80],[500,90],[520,400],[140,420]],
  "type": "restricted"
}
```

---

## 11. 파이썬 프로젝트 파일 구조 추천(Pycharm 기준)

단일 레포를 추천(캡스톤은 단순해야 운영이 편함)

```text
safety-cctv-ai/
  README.md
  pyproject.toml            # 또는 requirements.txt
  .env.example
  configs/
    default.yaml
    cameras.yaml
  src/
    safety_ai/
      __init__.py
      main.py                # 엔트리포인트
      config.py              # 설정 로딩
      logging_setup.py

      ingest/
        rtsp_reader.py       # RTSP 수신, 프레임 드랍 정책
        frame_queue.py

      vision/
        detector.py          # 사람 검출 래퍼
        ppe.py               # PPE 판별
        pose.py              # 포즈 추정
        tracker.py           # ByteTrack/DeepSORT 래퍼

      rules/
        zones.py             # 폴리곤 inside 체크
        risk_rules.py        # fall, lying 등 룰
        dedup.py             # 이벤트 중복 억제

      events/
        schema.py            # 이벤트 데이터 모델
        emitter.py           # 백엔드 전송(REST)
        snapshot.py          # 스냅샷 저장

      clients/
        backend_api.py       # /events POST, /zones GET 등
        auth.py

      utils/
        timeutil.py
        imageutil.py
        metrics.py

  scripts/
    run_local.sh
    benchmark.py
    export_snapshot.py
  tests/
    test_zones.py
    test_dedup.py
  data/
    samples/                 # 샘플 영상, 샘플 스냅샷
  outputs/
    snapshots/
    logs/
```

구조 포인트

* vision 은 모델, 추적 같은 순수 CV 기능
* rules 는 비즈니스 룰(위험 판단, 구역, 중복 억제)
* events 는 이벤트 생성과 전송, 저장
* ingest 는 RTSP 입력을 안정적으로 처리

---

## 12. 개발 순서(성공 확률 높은 MVP 로드맵)

### 1주차 MVP

* RTSP 수신해서 프레임을 안정적으로 읽기
* 사람 검출만 하고 bbox를 화면에 그려서 확인
* 이벤트는 콘솔 출력만

### 2주차

* 위험구역 폴리곤 설정 데이터 연동
* 침입 이벤트 생성
* 백엔드로 이벤트 POST 연동

### 3주차

* 추적(track_id) 붙여서 중복 억제 적용
* 스냅샷 저장, 프론트에서 이벤트 클릭 시 이미지 보기

### 4주차

* PPE 1~2종(안전모, 조끼) 판별 추가
* PPE 미착용 이벤트

### 5주차

* 포즈 추정 붙여서 넘어짐 또는 누움 감지(룰 기반)
* false alarm 줄이는 쿨다운, 연속 프레임 조건 추가

### 6주차

* IoT 연동(MQTT)로 사이렌 트리거
* 데모 시나리오 완성(침입, 미착용, 쓰러짐)

---

## 13. 성능과 안정성 팁(캡스톤에서 체감이 큼)

* 프레임 전부 처리하지 말고 일정 간격만 처리

  * 예: 30fps 들어와도 10fps만 추론
* 모델은 작은 버전으로 시작
* 추적을 넣으면 이벤트 신뢰도가 확 올라감
* 이벤트는 즉시 울리지 말고 조건을 둠

  * 예: 침입은 3프레임 연속 inside 일 때
  * PPE 미착용은 1초 동안 5프레임 중 4프레임 이상 미착용일 때
* 로그를 남겨야 디버깅이 가능

  * 카메라별 fps, 드랍률, 평균 추론시간, 이벤트 발생 수

---

## 14. 데모 시나리오 예시(발표에 좋음)

* 시나리오 A: 위험구역 진입

  * 사람이 폴리곤 안으로 들어옴
  * 프론트 팝업 + 스냅샷 + IoT 경광등

* 시나리오 B: 안전모 미착용

  * 사람이 감지됨
  * 안전모 없음 판정이 일정 시간 유지
  * 경고 이벤트 발생

* 시나리오 C: 쓰러짐 감지

  * 포즈 기반으로 넘어짐 감지
  * 5초 이상 누워있으면 HIGH severity로 승격

---

## 15. 너에게 당장 필요한 To Do 리스트

1. 팀과 합의 문서 3개 만들기

* 이벤트 JSON 스펙
* zone 좌표 포맷
* RTSP 스트림 규격(해상도, fps, 코덱)

2. 네 로컬에서 RTSP 입력을 안정적으로 받는 코드부터 만들기

* 끊김 재연결
* 프레임 드랍 정책
* 타임스탬프 관리

3. 사람 검출 + 추적 + 침입 감지까지를 1차 완주

* 이게 되면 나머지는 기능 추가 형태로 확장 가능

4. 라이선스 문서화 습관 들이기

* 사용한 모델, 코드, 데이터셋, 버전 기록
* 캡스톤 보고서에 그대로 넣으면 점수도 잘 나옴
